{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto I \n",
    "* Monica Alfaro Parrales\n",
    "* Adrián Ramírez Mattey\n",
    "* Gilberth Rodríguez Mejías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset \"Pima Indians Diabetes Database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "print(\"data head\")\n",
    "display(data.head())\n",
    "print(\"data describe\")\n",
    "display(data.describe())\n",
    "display(data)\n",
    "\n",
    "X = data[[\n",
    "    'Pregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DiabetesPedigreeFunction',\n",
    "    'Age'\n",
    "]].values\n",
    "\n",
    "y = data['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de datos\n",
    "1 = Diabetes\n",
    "\n",
    "0 = No diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos de dispersión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Pregnancies', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Embarazos\")\n",
    "plt.xlabel(\"Embarazos\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Glucose', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Glucosa\")\n",
    "plt.xlabel(\"Glucosa\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BloodPressure', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Presión Arterial\")\n",
    "plt.xlabel(\"Presión arterial\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='SkinThickness', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Grosor de la piel\")\n",
    "plt.xlabel(\"Grosor de la piel\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Insulin', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Insulina\")\n",
    "plt.xlabel(\"Insulina\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BMI', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de BMI\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='DiabetesPedigreeFunction', y='Age', hue='Outcome', data=data, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Diabetes Pedigree\")\n",
    "plt.xlabel(\"Diabetes Pedigree\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la cantidad de datos con Outcome 1 y 0\n",
    "outcome_counts = data['Outcome'].value_counts()\n",
    "\n",
    "# Crear el histograma\n",
    "plt.bar(outcome_counts.index, outcome_counts.values, color=['blue', 'green'])\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.title('Cantidad de datos por tipo de Outcome')\n",
    "plt.xticks([0, 1], ['Outcome 0', 'Outcome 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGroup = data.groupby('Outcome')\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['Pregnancies'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['Pregnancies'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - Embarazos')\n",
    "plt.xlabel('Embarazos')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['Glucose'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['Glucose'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - Glucosa')\n",
    "plt.xlabel('Glucosa')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['Insulin'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['Insulin'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - Insulina')\n",
    "plt.xlabel('Insulina')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['BMI'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['BMI'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - BMI')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['DiabetesPedigreeFunction'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['DiabetesPedigreeFunction'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - DiabetesPedigreeFunction')\n",
    "plt.xlabel('DiabetesPedigreeFunction')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(dataGroup['Age'].get_group(0), bins=30, alpha=0.5, label='0')\n",
    "plt.hist(dataGroup['Age'].get_group(1), bins=30, alpha=0.5, label='1')\n",
    "plt.title('Histograma de Diabetes - Edad')\n",
    "plt.xlabel('Edad')\n",
    "plt.ylabel('Personas')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de outliers y división del dataset\n",
    "Se procede a hacer uso de IQR, el rango intercuartil (IQR) es la diferencia entre el percentil 75 y el 25 de los datos. Es una medida de dispersión similar a la desviación estándar o la varianza, pero es mucho más robusta frente a valores atípicos. Posteriormente, se hace un shuffle de la data para asegurar una mayor distribución y se divide el dataset en un 80% training y 20% testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el IQR\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifica los outliers\n",
    "outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Encuentra las filas que tienen al menos un outlier\n",
    "outliers_indices = outliers.any(axis=1)\n",
    "\n",
    "# Muestra las filas con outliers\n",
    "print(\"Filas con outliers:\")\n",
    "display(data[outliers_indices])\n",
    "\n",
    "# Elimina los outliers\n",
    "data_sin_outliers = data[~outliers_indices]\n",
    "\n",
    "# Muestra el conjunto de datos sin outliers\n",
    "print(\"Data sin outliers:\")\n",
    "display(data_sin_outliers)\n",
    "\n",
    "# Estadísticas del conjunto de datos sin outliers\n",
    "print(\"Data sin outliers\")\n",
    "display(data_sin_outliers.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos de dispersión posterior a eliminación de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Pregnancies', y='Age', hue='Outcome', data=data_sin_outliers, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Embarazos (no outliers)\")\n",
    "plt.xlabel(\"Embarazos\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Glucose', y='Age', hue='Outcome', data=data_sin_outliers )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Glucosa (no outliers)\")\n",
    "plt.xlabel(\"Glucosa\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BloodPressure', y='Age', hue='Outcome', data=data_sin_outliers, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Presión Arterial (no outliers)\")\n",
    "plt.xlabel(\"Presión arterial\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='SkinThickness', y='Age', hue='Outcome', data=data_sin_outliers, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Grosor de la piel (no outliers)\")\n",
    "plt.xlabel(\"Grosor de la piel\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Insulin', y='Age', hue='Outcome', data=data_sin_outliers )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Insulina (no outliers)\")\n",
    "plt.xlabel(\"Insulina\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='BMI', y='Age', hue='Outcome', data=data_sin_outliers)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de BMI (no outliers)\")\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='DiabetesPedigreeFunction', y='Age', hue='Outcome', data=data_sin_outliers, )\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Diabetes Pedigree (no outliers)\")\n",
    "plt.xlabel(\"Diabetes Pedigree\")\n",
    "plt.ylabel(\"Edad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualizacion de datos en 0 y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Media de los datos sin outliers\n",
    "Pregnancies\tGlucose\t    BloodPressure\tSkinThickness\tInsulin\t    BMI\t        DiabetesPedigreeFunction\tAge\n",
    "3.804382\t119.112676\t72.120501\t    20.563380\t    65.931142\t32.00579\t0.429177\t                32.715180\n",
    "'''\n",
    "data_sin_ceros = data_sin_outliers.copy()\n",
    "data_sin_ceros.loc[data_sin_ceros['Pregnancies'] == 0, 'Pregnancies'] = 4 #Numeros enteros en el data set se rendondea 3.804382 a 4\n",
    "data_sin_ceros.loc[data_sin_ceros['Glucose'] == 0, 'Glucose'] = 120 #Numeros enteros en el data set se rendondea  119.112676 a 120\n",
    "data_sin_ceros.loc[data_sin_ceros['BloodPressure'] == 0, 'BloodPressure'] = 72 #Numeros enteros en el data set se rendondea 72.120501 a 72\n",
    "data_sin_ceros.loc[data_sin_ceros['SkinThickness'] == 0, 'SkinThickness'] = 21 #Numeros enteros en el data set se rendondea 20.563380 a 21\n",
    "data_sin_ceros.loc[data_sin_ceros['Insulin'] == 0, 'Insulin'] = 66 #Numeros enteros en el data set se rendondea 65.931142 a 80\n",
    "data_sin_ceros.loc[data_sin_ceros['BMI'] == 0, 'BMI'] = 32.0 #Numeros enteros en el data set se rendondea 32.00579 a 32.0\n",
    "data_sin_ceros.loc[data_sin_ceros['DiabetesPedigreeFunction'] == 0, 'DiabetesPedigreeFunction'] = 0.429 #Numeros enteros en el data set se rendondea 0.429177 a 0.429\n",
    "print(\"Data sin ceros\")\n",
    "display(data_sin_ceros.head())\n",
    "\n",
    "'''\n",
    "Normalizacion de los datos\n",
    "Minimos\n",
    "Pregnancies\tGlucose\t    BloodPressure\tSkinThickness\tInsulin\t    BMI\t        DiabetesPedigreeFunction\tAge\n",
    "0.000000\t44.000000\t38.000000\t    0.000000\t    0.000000\t18.20000\t0.078000\t                21.000000\n",
    "\n",
    "Maximos\n",
    "Pregnancies\tGlucose\t    BloodPressure\tSkinThickness\tInsulin\t    BMI\t        DiabetesPedigreeFunction\tAge\n",
    "13.000000\t198.000000\t106.000000\t    60.000000\t    318.000000\t50.00000\t1.191000\t                66.000000\t\n",
    "'''\n",
    "data_normalizada = data_sin_ceros.copy()\n",
    "data_normalizada['Pregnancies'] = (data_normalizada['Pregnancies'] - data_normalizada['Pregnancies'].min()) / (data_normalizada['Pregnancies'].max() - data_normalizada['Pregnancies'].min())\n",
    "data_normalizada['Glucose'] = (data_normalizada['Glucose'] - data_normalizada['Glucose'].min()) / (data_normalizada['Glucose'].max() - data_normalizada['Glucose'].min())\n",
    "data_normalizada['BloodPressure'] = (data_normalizada['BloodPressure'] - data_normalizada['BloodPressure'].min()) / (data_normalizada['BloodPressure'].max() - data_normalizada['BloodPressure'].min())\n",
    "data_normalizada['SkinThickness'] = (data_normalizada['SkinThickness'] - data_normalizada['SkinThickness'].min()) / (data_normalizada['SkinThickness'].max() - data_normalizada['SkinThickness'].min())\n",
    "data_normalizada['Insulin'] = (data_normalizada['Insulin'] - data_normalizada['Insulin'].min()) / (data_normalizada['Insulin'].max() - data_normalizada['Insulin'].min())\n",
    "data_normalizada['BMI'] = (data_normalizada['BMI'] - data_normalizada['BMI'].min()) / (data_normalizada['BMI'].max() - data_normalizada['BMI'].min())\n",
    "data_normalizada['DiabetesPedigreeFunction'] = (data_normalizada['DiabetesPedigreeFunction'] - data_normalizada['DiabetesPedigreeFunction'].min()) / (data_normalizada['DiabetesPedigreeFunction'].max() - data_normalizada['DiabetesPedigreeFunction'].min())\n",
    "data_normalizada['Age'] = (data_normalizada['Age'] - data_normalizada['Age'].min()) / (data_normalizada['Age'].max() - data_normalizada['Age'].min())\n",
    "print(\"Data normalizada\")\n",
    "display(data_normalizada.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de dividir los datos, barajamos el dataset de forma aleatoria para mayor diversidad de datos\n",
    "dataset_shuffled = shuffle(data_sin_outliers, random_state=20)  # random_state para reproducibilidad\n",
    "dataset_shuf_sin_ceros = shuffle(data_sin_ceros, random_state=20)  # random_state para reproducibilidad\n",
    "dataset_shuf_normalizada = shuffle(data_normalizada, random_state=20)  # random_state para reproducibilidad\n",
    "# random_state = 50 malos resultados en el modelo || random_state = 20 buenos resultados en el modelo\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "p_train = 0.8  # Porcentaje de training set\n",
    "train_index = int(len(dataset_shuffled) * p_train)\n",
    "\n",
    "dataFrameTraining = dataset_shuffled[:train_index]\n",
    "dataFrameTraining_sin_ceros = dataset_shuf_sin_ceros[:train_index]\n",
    "dataFrameTraining_normalizada = dataset_shuf_normalizada[:train_index]\n",
    "\n",
    "dataFrameTesting = dataset_shuffled[train_index:]\n",
    "dataFrameTesting_sin_ceros = dataset_shuf_sin_ceros[train_index:]\n",
    "dataFrameTesting_normalizada = dataset_shuf_normalizada[train_index:]\n",
    "\n",
    "print(\"Ejemplos usados para entrenar: \", len(dataFrameTraining))\n",
    "print(\"Ejemplos usados para test: \", len(dataFrameTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para dividir los datos en x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyDivide(dataFrameTraining, dataFrameTesting):\n",
    "    x_train = dataFrameTraining[[\n",
    "    'Pregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'SkinThickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DiabetesPedigreeFunction',\n",
    "    'Age'\n",
    "    ]].values\n",
    "\n",
    "    y_train = dataFrameTraining['Outcome']\n",
    "\n",
    "    x_test = dataFrameTesting[[\n",
    "        'Pregnancies',\n",
    "        'Glucose',\n",
    "        'BloodPressure',\n",
    "        'SkinThickness',\n",
    "        'Insulin',\n",
    "        'BMI',\n",
    "        'DiabetesPedigreeFunction',\n",
    "        'Age'\n",
    "    ]].values\n",
    "\n",
    "    y_test = dataFrameTesting['Outcome']\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para graficar las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAccGraph(lst):\n",
    "    acc = []\n",
    "    for i in range(0, len(lst)):\n",
    "        acc.append(lst[i]['acc'])\n",
    "    plt.plot(acc)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def printPrecGraph(lst):\n",
    "    prec = []\n",
    "    for i in range(0, len(lst)):\n",
    "        prec.append(lst[i]['prec'])\n",
    "    plt.plot(prec)\n",
    "    plt.title('Precision')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def printRecGraph(lst):\n",
    "    rec = []\n",
    "    for i in range(0, len(lst)):\n",
    "        rec.append(lst[i]['rec'])\n",
    "    plt.plot(rec)\n",
    "    plt.title('Recall')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def printF1Graph(lst):\n",
    "    f1 = []\n",
    "    for i in range(0, len(lst)):\n",
    "        f1.append(lst[i]['f1'])\n",
    "    plt.plot(f1)\n",
    "    plt.title('F1')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('F1')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def printAucGraph(lst):\n",
    "    auc = []\n",
    "    for i in range(0, len(lst)):\n",
    "        auc.append(lst[i]['auc'])\n",
    "    plt.plot(auc)\n",
    "    plt.title('AUC')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def printROC(roc, k):\n",
    "    plt.plot(roc[0], roc[1], label=f\"K = {k}\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve k = {k}\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def printConfusionMatrix(cm, k):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, cmap='bwr')\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='white', fontsize=20)\n",
    "    plt.xticks([0,1], ['No diabetico', 'Diabetico'])\n",
    "    plt.yticks([0,1], ['No diabetico', 'Diabetico'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f\"Matriz Confusión k = {k}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN y metricas del KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN con data set sin outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining, dataFrameTesting)\n",
    "lst = []\n",
    "    \n",
    "for i in range(1,101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_curve(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    matrizConfusion = confusion_matrix(y_test, y_pred)\n",
    "    lst.append({\"k\": i, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"roc\": roc, \"matrizConfusion\": matrizConfusion})\n",
    "\n",
    "\n",
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"k = {maxAcc['k']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"k = {maxPrec['k']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"k = {maxRec['k']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"k = {maxF1['k']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max AUC\")\n",
    "print(f\"k = {maxAuc['k']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)\n",
    "\n",
    "printROC(maxAcc['roc'], maxAcc['k'])\n",
    "printROC(maxPrec['roc'], maxPrec['k'])\n",
    "printROC(maxRec['roc'], maxRec['k'])\n",
    "printROC(maxF1['roc'], maxF1['k'])\n",
    "printROC(maxAuc['roc'], maxAuc['k'])\n",
    "\n",
    "printConfusionMatrix(maxAcc['matrizConfusion'], maxAcc['k'])\n",
    "printConfusionMatrix(maxPrec['matrizConfusion'], maxPrec['k'])\n",
    "printConfusionMatrix(maxRec['matrizConfusion'], maxRec['k'])\n",
    "printConfusionMatrix(maxF1['matrizConfusion'], maxF1['k'])\n",
    "printConfusionMatrix(maxAuc['matrizConfusion'], maxAuc['k'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN data set sin ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_sin_ceros, dataFrameTesting_sin_ceros)\n",
    "lst = []\n",
    "\n",
    "for i in range(1,101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_curve(y_test, y_pred)\n",
    "    matrizConfusion = confusion_matrix(y_test, y_pred)\n",
    "    lst.append({\"k\": i, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"roc\": roc, \"matrizConfusion\": matrizConfusion})\n",
    "\n",
    "\n",
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"k = {maxAcc['k']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"k = {maxPrec['k']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"k = {maxRec['k']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"k = {maxF1['k']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max AUC\")\n",
    "print(f\"k = {maxAuc['k']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)\n",
    "\n",
    "printROC(maxAcc['roc'], maxAcc['k'])\n",
    "printROC(maxPrec['roc'], maxPrec['k'])\n",
    "printROC(maxRec['roc'], maxRec['k'])\n",
    "printROC(maxF1['roc'], maxF1['k'])\n",
    "printROC(maxAuc['roc'], maxAuc['k'])\n",
    "\n",
    "printConfusionMatrix(maxAcc['matrizConfusion'], maxAcc['k'])\n",
    "printConfusionMatrix(maxPrec['matrizConfusion'], maxPrec['k'])\n",
    "printConfusionMatrix(maxRec['matrizConfusion'], maxRec['k'])\n",
    "printConfusionMatrix(maxF1['matrizConfusion'], maxF1['k'])\n",
    "printConfusionMatrix(maxAuc['matrizConfusion'], maxAuc['k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN data set normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_normalizada, dataFrameTesting_normalizada)\n",
    "lst = []\n",
    "\n",
    "for i in range(1,101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(x_train, y_train)\n",
    "    y_pred = knn.predict(x_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_curve(y_test, y_pred)\n",
    "    matrizConfusion = confusion_matrix(y_test, y_pred)\n",
    "    lst.append({\"k\": i, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"roc\": roc, \"matrizConfusion\": matrizConfusion})\n",
    "\n",
    "\n",
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"k = {maxAcc['k']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"k = {maxPrec['k']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"k = {maxRec['k']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"k = {maxF1['k']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max AUC\")\n",
    "print(f\"k = {maxAuc['k']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)\n",
    "\n",
    "printROC(maxAcc['roc'], maxAcc['k'])\n",
    "printROC(maxPrec['roc'], maxPrec['k'])\n",
    "printROC(maxRec['roc'], maxRec['k'])\n",
    "printROC(maxF1['roc'], maxF1['k'])\n",
    "printROC(maxAuc['roc'], maxAuc['k'])\n",
    "\n",
    "printConfusionMatrix(maxAcc['matrizConfusion'], maxAcc['k'])\n",
    "printConfusionMatrix(maxPrec['matrizConfusion'], maxPrec['k'])\n",
    "printConfusionMatrix(maxRec['matrizConfusion'], maxRec['k'])\n",
    "printConfusionMatrix(maxF1['matrizConfusion'], maxF1['k'])\n",
    "printConfusionMatrix(maxAuc['matrizConfusion'], maxAuc['k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logísticas y métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sin outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datasets de entrenamiento y prueba\n",
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining, dataFrameTesting)\n",
    "\n",
    "# hiper parámetros:\n",
    "# C: Inverso de la fuerza de regulación,\n",
    "# max_iter: cantidad máxima de iteraciones para convergencia del solver,\n",
    "# penalty: la norma de penalidad del algoritmo\n",
    "# solver: tipo de algoritmo para la optimización del problema (tiene que ir de la mano con el tipo de penalizador)\n",
    "\n",
    "lr_c = 100\n",
    "lr_max_iter = 1000\n",
    "\n",
    "lr_model = LogisticRegression(C=lr_c, max_iter=lr_max_iter, )\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "lr_pred = lr_model.predict(x_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_prec = precision_score(y_test, lr_pred)\n",
    "lr_rec = recall_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_roc = roc_curve(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "\n",
    "lr_performance = {'c': lr_c, 'max_iter': lr_max_iter, 'acc': lr_acc, 'prec': lr_prec, 'rec': lr_rec, 'f1': lr_f1, 'auc': lr_auc, 'roc': lr_roc}\n",
    "\n",
    "display(lr_performance)\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "plot_data = {\n",
    "    'Accuracy': [lr_acc],\n",
    "    'Precision': [lr_prec],\n",
    "    'Recall': [lr_rec],\n",
    "    'F1-Score': [lr_f1],\n",
    "    'AUC': [lr_auc]\n",
    "}\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in plot_df.columns:\n",
    "    ax.plot(plot_df[col], plot_df[col], marker='X', label=col, ms=10.0)\n",
    "\n",
    "ax.set_title('Evaluación del modelo Logistic Regression')\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobación hiperparámetro C\n",
    "# Cambiar periódicamente el valor C para ver si afecta el desempeño\n",
    "\n",
    "log_list = []\n",
    "\n",
    "acc_list = []\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "f1_list = []\n",
    "roc_list = []\n",
    "auc_list = []\n",
    "\n",
    "#c_range = list(range(100,10100, 100))\n",
    "c_range = np.linspace(0.01, 1.0, 100)\n",
    "\n",
    "for c_val in c_range:\n",
    "    log_reg = LogisticRegression(C=c_val, max_iter=lr_max_iter, )\n",
    "    log_reg.fit(x_train, y_train)\n",
    "    log_pred = log_reg.predict(x_test)\n",
    "    \n",
    "    log_acc = accuracy_score(y_test, log_pred)\n",
    "    log_prec = precision_score(y_test, log_pred)\n",
    "    log_rec = recall_score(y_test, log_pred)\n",
    "    log_f1 = f1_score(y_test, log_pred)\n",
    "    \n",
    "    log_roc = roc_curve(y_test, log_pred)\n",
    "    log_auc = roc_auc_score(y_test, log_pred)\n",
    "    \n",
    "    log_performance = {'c': c_val, 'max_iter': lr_max_iter, 'acc': log_acc, 'prec': log_prec, 'rec': log_rec, 'f1': log_f1, 'auc': log_auc, 'roc': log_roc}\n",
    "    \n",
    "    log_list.append(log_performance)\n",
    "    acc_list.append(log_acc)\n",
    "    prec_list.append(log_prec)\n",
    "    rec_list.append(log_rec)\n",
    "    f1_list.append(log_f1)\n",
    "    roc_list.append(log_roc)\n",
    "    auc_list.append(log_auc)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(c_range, acc_list, label='Accuracy', )\n",
    "ax.plot(c_range, prec_list, label='Precision', )\n",
    "ax.plot(c_range, rec_list, label='Recall', )\n",
    "ax.plot(c_range, f1_list, label='F1 Score', )\n",
    "ax.set_title('Métricas de Regresión Logística')\n",
    "ax.set_xlabel('Valor de C (inverso fuerza regulación)')\n",
    "ax.set_ylabel('Valor de la métrica')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación Estricto vs Laxo\n",
    "# Análisis concreto de un modelo con posible overfitting vs más laxo y con posible underfitting\n",
    "\n",
    "# Se puede entender un valor de C más bajo (cercano a 0) como un modelo más estricto\n",
    "# Y un valor de C más alto como un modelo más laxo/permisivo\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# [...] Like in support vector machines, smaller values specify stronger regularization.\n",
    "c_values = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "lr_max_iter = 1000\n",
    "\n",
    "model_performances = {'C': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "\n",
    "for i in range(len(c_values)):\n",
    "    lr_model = LogisticRegression(C=c_values[i], max_iter=lr_max_iter, )\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    # Predicción y evaluación\n",
    "    lr_pred = lr_model.predict(x_test)\n",
    "    lr_acc = accuracy_score(y_test, lr_pred)\n",
    "    lr_prec = precision_score(y_test, lr_pred)\n",
    "    lr_rec = recall_score(y_test, lr_pred)\n",
    "    lr_f1 = f1_score(y_test, lr_pred)\n",
    "    lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "    lr_roc = roc_curve(y_test, lr_pred)\n",
    "    \n",
    "    model_performances['C'].append(c_values[i])\n",
    "    model_performances['Accuracy'].append(lr_acc)\n",
    "    model_performances['Precision'].append(lr_prec)\n",
    "    model_performances['Recall'].append(lr_rec)\n",
    "    model_performances['F1-Score'].append(lr_f1)\n",
    "    model_performances['AUC'].append(lr_auc)\n",
    "    model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "\n",
    "print('Performances for C values')\n",
    "display(model_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in model_df.columns[1:6]:\n",
    "    ax.plot(model_df['C'], model_df[col], marker='X', label=col, ms=10.0)\n",
    "\n",
    "ax.set_title('Evaluación del modelo Logistic Regression')\n",
    "ax.set_xlabel('Valor C')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot para solo evaluar F1-Score\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(model_df['C'], model_df['F1-Score'], marker='X', label='F1-Score', ms=10.0)\n",
    "\n",
    "ax.set_title('Evaluación F1-Score Logistic Regression')\n",
    "ax.set_xlabel('Valor C')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con diferentes 'Solucionadores' para el modelo\n",
    "# Análisis sobre los diferentes solucionadores ofrecidos por la librería\n",
    "\n",
    "# Los diferentes solucionadores tienen diferentes características, los que ofrece la librería son los de la lista:\n",
    "# solver {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, default='lbfgs'\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones;\n",
    "# For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss;\n",
    "# 'liblinear' is limited to one-versus-rest schemes.\n",
    "# 'newton-cholesky' is a good choice for n_samples >> n_features\n",
    "solver_list = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "lr_max_iter = 10000\n",
    "selected_C = 0.2 # Por los resultados de la prueba anterior\n",
    "\n",
    "model_performances = {'Solver': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "\n",
    "for i in range(len(solver_list)):\n",
    "    lr_model = LogisticRegression(C=selected_C, max_iter=lr_max_iter, solver=solver_list[i])\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    # Predicción y evaluación\n",
    "    lr_pred = lr_model.predict(x_test)\n",
    "    lr_acc = accuracy_score(y_test, lr_pred)\n",
    "    lr_prec = precision_score(y_test, lr_pred)\n",
    "    lr_rec = recall_score(y_test, lr_pred)\n",
    "    lr_f1 = f1_score(y_test, lr_pred)\n",
    "    lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "    lr_roc = roc_curve(y_test, lr_pred)\n",
    "    \n",
    "    model_performances['Solver'].append(solver_list[i])\n",
    "    model_performances['Accuracy'].append(lr_acc)\n",
    "    model_performances['Precision'].append(lr_prec)\n",
    "    model_performances['Recall'].append(lr_rec)\n",
    "    model_performances['F1-Score'].append(lr_f1)\n",
    "    model_performances['AUC'].append(lr_auc)\n",
    "    model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "\n",
    "print('Performances for each Solver')\n",
    "display(model_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in model_df.columns[1:6]:\n",
    "    ax.plot(model_df['Solver'], model_df[col], marker='X', label=col, ms=10.0)\n",
    "\n",
    "ax.set_title('Evaluación del modelo Logistic Regression')\n",
    "ax.set_xlabel('Solucionador')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot para solo evaluar F1-Score\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(model_df['Solver'], model_df['F1-Score'], marker='X', label='F1-Score', ms=10.0)\n",
    "\n",
    "ax.set_title('Evaluación F1-Score Logistic Regression')\n",
    "ax.set_xlabel('Solucionador')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con diferentes 'penalizadores' en el modelo\n",
    "\n",
    "# Debido a que el modelo de la librería solo puede usar algunas combinaciones\n",
    "# en específico de solucionadores con penalizadores, se usarán los tres\n",
    "# mejores rendimientos del experimento anterior: lbfgs, newton-cg y newton-cholesky\n",
    "\n",
    "penalty_list = ['l2', None]\n",
    "solver_list = ['lbfgs', 'newton-cg', 'newton-cholesky']\n",
    "lr_max_iter = 10000\n",
    "selected_C = 0.2 # Por los resultados de la prueba anterior\n",
    "\n",
    "model_performances = {'Solver': [], 'Penalty': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "\n",
    "for i in range(len(penalty_list)):\n",
    "    for j in range(len(solver_list)):\n",
    "        if penalty_list[i] == None:\n",
    "            # Tira warning si se configura C value con ninguna penalidad\n",
    "            lr_model = LogisticRegression(max_iter=lr_max_iter, solver=solver_list[j], penalty=penalty_list[i])\n",
    "        else:\n",
    "            lr_model = LogisticRegression(C=selected_C, max_iter=lr_max_iter, solver=solver_list[j], penalty=penalty_list[i])\n",
    "        lr_model.fit(x_train, y_train)\n",
    "        # Predicción y evaluación\n",
    "        lr_pred = lr_model.predict(x_test)\n",
    "        lr_acc = accuracy_score(y_test, lr_pred)\n",
    "        lr_prec = precision_score(y_test, lr_pred)\n",
    "        lr_rec = recall_score(y_test, lr_pred)\n",
    "        lr_f1 = f1_score(y_test, lr_pred)\n",
    "        lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "        lr_roc = roc_curve(y_test, lr_pred)\n",
    "        \n",
    "        model_performances['Solver'].append(solver_list[j])\n",
    "        model_performances['Penalty'].append(penalty_list[i])\n",
    "        model_performances['Accuracy'].append(lr_acc)\n",
    "        model_performances['Precision'].append(lr_prec)\n",
    "        model_performances['Recall'].append(lr_rec)\n",
    "        model_performances['F1-Score'].append(lr_f1)\n",
    "        model_performances['AUC'].append(lr_auc)\n",
    "        model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "\n",
    "print('Performances for each Solver-Penalty pair')\n",
    "display(model_df)\n",
    "\n",
    "\n",
    "# Hacer 2 sub-datasets para los que tengan el penalty l2 y None\n",
    "df_l2 = model_df[model_df['Penalty'] == 'l2']\n",
    "df_None = model_df[pd.isnull(model_df['Penalty'])]\n",
    "\n",
    "print('2 Separate datasets:')\n",
    "display(df_l2)\n",
    "display(df_None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in df_None.columns[2:6]:\n",
    "    ax.plot(df_None['Solver'], df_None[col], marker='X', label=f'{col} Penalty: None', ms=10.0, alpha=1.0)\n",
    "for col in df_l2.columns[2:6]:\n",
    "    ax.plot(df_l2['Solver'], df_l2[col], marker='*', label=f'{col} Penalty: L2', ms=10.0, alpha=1.0)\n",
    "\n",
    "ax.set_title('Evaluación del modelo Logistic Regression')\n",
    "ax.set_xlabel('Solucionador')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot para solo evaluar F1-Score\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(df_None['Solver'], df_None['F1-Score'], marker='X', label='F1-Score Penalty: None', ms=10.0, alpha=1.0)\n",
    "ax.plot(df_l2['Solver'], df_l2['F1-Score'], marker='*', label='F1-Score Penalty: L2', ms=10.0, alpha=1.0)\n",
    "\n",
    "ax.set_title('Evaluación F1-Score Logistic Regression')\n",
    "ax.set_xlabel('Solucionador')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sin ceros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_sin_ceros, dataFrameTesting_sin_ceros)\n",
    "\n",
    "\n",
    "# Mejores resultados anteriormente son:\n",
    "# C = 0.2\n",
    "# [VANITY] max_iter > 500\n",
    "# Penalty: l2\n",
    "# Solver: lbfgs\n",
    "\n",
    "\n",
    "\n",
    "model_performances = {'Solver': [], 'Penalty': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=500, solver='lbfgs', penalty='l2')\n",
    "lr_model.fit(x_train, y_train)\n",
    "# Predicción y evaluación\n",
    "lr_pred = lr_model.predict(x_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_prec = precision_score(y_test, lr_pred)\n",
    "lr_rec = recall_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_roc = roc_curve(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "\n",
    "\n",
    "model_performances['Solver'].append('lbfgs')\n",
    "model_performances['Penalty'].append('l2')\n",
    "model_performances['Accuracy'].append(lr_acc)\n",
    "model_performances['Precision'].append(lr_prec)\n",
    "model_performances['Recall'].append(lr_rec)\n",
    "model_performances['F1-Score'].append(lr_f1)\n",
    "model_performances['AUC'].append(lr_auc)\n",
    "model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "\n",
    "print('Performance for non-zero values')\n",
    "display(model_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in model_df.columns[2:6]:\n",
    "    ax.plot(model_df[col], model_df[col], marker='X', label=col, ms=10.0)\n",
    "ax.set_title('Evaluación del modelo Logistic Regression (sin ceros)')\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_normalizada, dataFrameTesting_normalizada)\n",
    "\n",
    "# Mejores resultados anteriormente son:\n",
    "# C = 0.2\n",
    "# [VANITY] max_iter > 500\n",
    "# Penalty: l2\n",
    "# Solver: lbfgs\n",
    "\n",
    "model_performances = {'Solver': [], 'Penalty': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=500, solver='lbfgs', penalty='l2')\n",
    "lr_model.fit(x_train, y_train)\n",
    "# Predicción y evaluación\n",
    "lr_pred = lr_model.predict(x_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "lr_prec = precision_score(y_test, lr_pred)\n",
    "lr_rec = recall_score(y_test, lr_pred)\n",
    "lr_f1 = f1_score(y_test, lr_pred)\n",
    "lr_roc = roc_curve(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "\n",
    "\n",
    "model_performances['Solver'].append('lbfgs')\n",
    "model_performances['Penalty'].append('l2')\n",
    "model_performances['Accuracy'].append(lr_acc)\n",
    "model_performances['Precision'].append(lr_prec)\n",
    "model_performances['Recall'].append(lr_rec)\n",
    "model_performances['F1-Score'].append(lr_f1)\n",
    "model_performances['AUC'].append(lr_auc)\n",
    "model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Histograma con las evaluaciones\n",
    "\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "\n",
    "print('Performance for non-zero values')\n",
    "display(model_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "for col in model_df.columns[2:6]:\n",
    "    ax.plot(model_df[col], model_df[col], marker='X', label=col, ms=10.0)\n",
    "ax.set_title('Evaluación del modelo Logistic Regression (sin ceros)')\n",
    "ax.set_xlabel('Métrica')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de los 3 procesamientos del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se van a comparar los 3 rendimientos diferentes, con cada dataset\n",
    "dataset_list = ['sin-outliers', 'sin-ceros', 'normalizado']\n",
    "confusion_list = []\n",
    "model_performances = {'Dataset': [], 'Accuracy': [], 'Precision': [],\n",
    "                      'Recall': [], 'F1-Score': [], 'AUC': [], 'ROC': []}\n",
    "for i in range(len(dataset_list)):\n",
    "    # Selecciona los datos de entrenamiento\n",
    "    match i:\n",
    "        case 0:\n",
    "            x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining, dataFrameTesting)\n",
    "            \n",
    "        case 1:\n",
    "            x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_sin_ceros, dataFrameTesting_sin_ceros)\n",
    "        case 2:\n",
    "            x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_normalizada, dataFrameTesting_normalizada)\n",
    "    lr_model = LogisticRegression(max_iter=500, solver='lbfgs', penalty='l2')\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    # Predicción y evaluación\n",
    "    lr_pred = lr_model.predict(x_test)\n",
    "    lr_acc = accuracy_score(y_test, lr_pred)\n",
    "    lr_prec = precision_score(y_test, lr_pred)\n",
    "    lr_rec = recall_score(y_test, lr_pred)\n",
    "    lr_f1 = f1_score(y_test, lr_pred)\n",
    "    lr_roc = roc_curve(y_test, lr_pred)\n",
    "    lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "    # Confusion Matrix\n",
    "    lr_cm = confusion_matrix(y_test, lr_pred)\n",
    "    confusion_list.append(lr_cm)\n",
    "    model_performances['Dataset'].append(dataset_list[i])\n",
    "    model_performances['Accuracy'].append(lr_acc)\n",
    "    model_performances['Precision'].append(lr_prec)\n",
    "    model_performances['Recall'].append(lr_rec)\n",
    "    model_performances['F1-Score'].append(lr_f1)\n",
    "    model_performances['AUC'].append(lr_auc)\n",
    "    model_performances['ROC'].append(lr_roc)\n",
    "\n",
    "\n",
    "\n",
    "# Plot con las evaluaciones\n",
    "model_df = pd.DataFrame(model_performances)\n",
    "print('Performance for 3 datasets')\n",
    "display(model_df)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "for col in model_df.columns[1:6]:\n",
    "    ax.plot(model_df['Dataset'], model_df[col], marker='X', label=col, ms=10.0)\n",
    "ax.set_title('Evaluación del modelo Logistic Regression (por procesamiento del dataset)')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Valor métrica')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Matrices de confusión\n",
    "cn = 1\n",
    "for cm in confusion_list:\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    im = ax.imshow(cm, cmap='bwr')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_xticks([0,1], ['No diabético', 'Diabético'])\n",
    "    ax.set_yticks([0,1], ['No diabético', 'Diabético'])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha='center', va='center', color='white', fontsize=20)\n",
    "    ax.set_title(f'Mapa de Confusión para DS: {dataset_list[cn]}')\n",
    "    ax.set_xlabel('Predicción clase')\n",
    "    ax.set_ylabel('Clase real')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    cn += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales y sus métricas\n",
    "Las redes neuronales se desarrollaron haciendo uso de la librería de Scikit Learn. \n",
    "Hiperparámetros:\n",
    "* hidden_layer_sizes tupla, length = n_layers - 2, default=(100,): El elemento i-ésimo representa el número de neuronas en la i-ésima capa oculta.\n",
    "* activation{“identity”, “logistic”, “tanh”, “relu”}, default=”relu”. Función de activación para la capa oculta.\n",
    "    * “identity”, activación no-op, útil para implementar el cuello de botella lineal, devuelve f(x) = x\n",
    "    * “logistic”, la función sigmoide logística, devuelve f(x) = 1 / (1 + exp(-x)).\n",
    "    * “tanh”, la función tangente hiperbólica, devuelve f(x) = tanh(x).\n",
    "    * “relu”, la función de unidad lineal rectificada, devuelve f(x) = max(0, x)\n",
    "* solver{“lbfgs”, “sgd”, “adam”}, default=”adam”: El solucionador para la optimización de la ponderación.\n",
    "    * “lbfgs” es un optimizador en la familia de los métodos cuasi-Newton.\n",
    "    *  “sgd” se refiere al descenso de gradiente estocástico.\n",
    "    *   “adam” se refiere a un optimizador basado en el gradiente estocástico propuesto por Kingma, Diederik y Jimmy Ba\n",
    "\n",
    "Nota: El solucionador por defecto “adam” funciona bastante bien en conjuntos de datos relativamente grandes (con miles de muestras de entrenamiento o más) en términos de tiempo de entrenamiento y puntuación de validación. Sin embargo, para conjuntos de datos pequeños, “lbfgs” puede converger más rápido y funcionar mejor.\n",
    "\n",
    "* max_iterint, default=200: Número máximo de iteraciones. El solucionador itera hasta la convergencia (determinada por “tol”) o este número de iteraciones. Para los solucionadores estocásticos (“sgd”, “adam”), ten en cuenta que esto determina el número de épocas (cuántas veces se utilizará cada punto de datos), no el número de pasos del gradiente.\n",
    "* random_state, entero, instancia de RandomState, default=None: Determina la generación de números aleatorios para la inicialización de las ponderaciones y el sesgo, la división de entrenamiento-prueba si se utiliza la parada anticipada, y el muestreo por lotes cuando solver=”sgd” o “adam”. Pasa un int para obtener resultados reproducibles a través de múltiples llamadas a la función.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación de diferentes cantidades de neuronas, redes y epochs | Activation = ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = xyDivide(dataFrameTraining_normalizada, dataFrameTesting_normalizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 4700, Redes = 1, Neuronas = 1-50\n",
    "La utilización de epochs menores o iguales que 4600 genera que en algunas ocasiones la red con más de 40 neuronas no converja, por lo que se utiliza la cantidad de epochs 4700. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = lbgfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='relu',\n",
    "        solver='lbgfs',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del mejor modelo\n",
    "hidden_layer_sizes=(i,),\n",
    "activation='relu',\n",
    "solver='adam',\n",
    "max_iter=4700,\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 11:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 5900, Redes = 1, Neuronas = 51-100\n",
    "La utilización de epochs menores o iguales que 5800 genera que algunas en ocasiones la red con más de 50 neuronas no converja, por lo que se utiliza la cantidad de epochs 5900. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizador = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(51, 101):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=5900,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizador = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(51, 101):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=5900,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del mejor modelo\n",
    "hidden_layer_sizes=(i,),\n",
    "activation='relu',\n",
    "solver='adam',\n",
    "max_iter=4700,\n",
    "random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 51:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 99:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 4700, Redes = 2, Neuronas = 1-50\n",
    "La utilización de epochs menores o iguales que 4600 genera que algunas en ocasiones la red con más de 40 neuronas no converja, por lo que se utiliza la cantidad de epochs 4700. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del mejor modelo\n",
    "hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=4700,\n",
    "        random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 18:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 5900, Redes = 2, Neuronas = 50-100\n",
    "La utilización de epochs menores o iguales que 5800 genera que algunas en ocasiones la red con más de 50 neuronas no converja, por lo que se utiliza la cantidad de epochs 5900. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=4700,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del modelo con mejores métricas\n",
    "hidden_layer_sizes=(i,i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=4700,\n",
    "        random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 18:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 5600, Redes = 3, Neuronas = 1-50\n",
    "La utilización de epochs menores o iguales que 5500 genera que algunas en ocasiones la red con más de 40 neuronas no converja, por lo que se utiliza la cantidad de epochs 5600. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,i,),\n",
    "        activation='relu',\n",
    "        solver='lbfgs',\n",
    "        max_iter=5600,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=5600,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del modelo con mejores métricas\n",
    "hidden_layer_sizes=(i,i,i,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=5600,\n",
    "        random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 23:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación de diferentes cantidades de neuronas, redes y epochs | Activation = logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 6800, Redes = 1, Neuronas = 1-50\n",
    "La utilización de epochs menores o iguales que 6700 genera que en algunas ocasiones la red con más de 40 neuronas no converja, por lo que se utiliza la cantidad de epochs 6800. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizador = lbgfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='logistic',\n",
    "        solver='lbfgs',\n",
    "        max_iter=6800,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizador = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='logistic',\n",
    "        solver='adam',\n",
    "        max_iter=6800,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales del modelo con mejor métricas\n",
    "hidden_layer_sizes=(i,),\n",
    "        activation='logistic',\n",
    "        solver='adam',\n",
    "        max_iter=6800,\n",
    "        random_state=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 30:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 6800, Redes = 1, Neuronas = 51-100\n",
    "La utilización de epochs menores o iguales que 6700 genera que algunas en ocasiones la red con más de 50 neuronas no converja, por lo que se utiliza la cantidad de epochs 6800. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(51, 101):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='logistic',\n",
    "        solver='lbfgs',\n",
    "        max_iter=6800,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(51, 101):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,),\n",
    "        activation='logistic',\n",
    "        solver='adam',\n",
    "        max_iter=6800,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 91:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epochs = 20000, Redes = 2, Neuronas = 1-50\n",
    "Para el modelo de 2 redes neuronales con 1 a 50 neuronas, es posible observar que en ocasiones el modelo tiene muchas dificultades para converger aún con 20,000 épocas y obteniendo resultados no mejores que con la función de activación ReLu, por lo que se considera más eficiente la función de activación ReLu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='logistic',\n",
    "        solver='lbfgs',\n",
    "        max_iter=20000,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para almacenar las métricas\n",
    "neuronas = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "roc_scores = []\n",
    "lst = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Iterar sobre el rango especificado\n",
    "for i in range(1, 51):\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(i,i,),\n",
    "        activation='logistic',\n",
    "        solver='lbfgs',\n",
    "        max_iter=20000,\n",
    "        random_state=123\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_list.append((i, y_pred))\n",
    "\n",
    "    # Calcular métricas\n",
    "    report = classification_report(y_test, y_pred, zero_division=1, output_dict=True)\n",
    "    accuracy_nn = accuracy_score(y_test, y_pred)\n",
    "    precision_nn = report['weighted avg']['precision']\n",
    "    recall_nn = report['weighted avg']['recall']\n",
    "    f1_nn = report['weighted avg']['f1-score']\n",
    "    roc_nn = roc_curve(y_test, y_pred)\n",
    "    auc_nn = roc_auc_score(y_test, y_pred)\n",
    "    lst.append({\"neu\": i, \"acc\": accuracy_nn, \"prec\": precision_nn, \"rec\": recall_nn, \"f1\": f1_nn, \"auc\": auc_nn, \"roc\": roc_nn})\n",
    "\n",
    "    # Almacenar las métricas en las listas\n",
    "    neuronas.append(i)\n",
    "    accuracy_scores.append(accuracy_nn)\n",
    "    precision_scores.append(precision_nn)\n",
    "    recall_scores.append(recall_nn)\n",
    "    f1_scores.append(f1_nn)\n",
    "    auc_scores.append(auc_nn)\n",
    "    roc_scores.append(roc_nn)\n",
    "\n",
    "    # Imprimir las métricas de cada iteración\n",
    "    #print(f'Neuronas: {i}, Redes: 1')\n",
    "    #print(f'Accuracy: {accuracy_nn:.2f}')\n",
    "    #print(f'Precision: {precision:.2f}')\n",
    "    #print(f'Recall: {recall:.2f}')\n",
    "    #print(f'F1: {f1:.2f}')\n",
    "\n",
    "# Graficar las métricas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(neuronas, accuracy_scores, label='Accuracy')\n",
    "plt.plot(neuronas, precision_scores, label='Precision')\n",
    "plt.plot(neuronas, recall_scores, label='Recall')\n",
    "plt.plot(neuronas, f1_scores, label='F1')\n",
    "plt.xlabel('Cantidad de neuronas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Métricas vs. Cantidad de neuronas')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Métricas individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAuc = max(lst, key=lambda x:x['auc'])\n",
    "maxPrec = max(lst, key=lambda x:x['prec'])\n",
    "maxRec = max(lst, key=lambda x:x['rec'])\n",
    "maxF1 = max(lst, key=lambda x:x['f1'])\n",
    "maxAcc = max(lst, key=lambda x:x['acc'])\n",
    "\n",
    "print(\"Max AUC\")\n",
    "print(f\"neu = {maxAuc['neu']} || AUC = {maxAuc['auc']} || Precision = {maxAuc['prec']} || Recall = {maxAuc['rec']} || F1 = {maxAuc['f1']} || Accuracy = {maxAuc['acc']}\")\n",
    "print(\"Max Precision\")\n",
    "print(f\"neu = {maxPrec['neu']} || AUC = {maxPrec['auc']} || Precision = {maxPrec['prec']} || Recall = {maxPrec['rec']} || F1 = {maxPrec['f1']} || Accuracy = {maxPrec['acc']}\")\n",
    "print(\"Max Recall\")\n",
    "print(f\"neu = {maxRec['neu']} || AUC = {maxRec['auc']} || Precision = {maxRec['prec']} || Recall = {maxRec['rec']} || F1 = {maxRec['f1']} || Accuracy = {maxRec['acc']}\")\n",
    "print(\"Max F1\")\n",
    "print(f\"neu = {maxF1['neu']} || AUC = {maxF1['auc']} || Precision = {maxF1['prec']} || Recall = {maxF1['rec']} || F1 = {maxF1['f1']} || Accuracy = {maxF1['acc']}\")\n",
    "print(\"Max Accuracy\")\n",
    "print(f\"neu = {maxAcc['neu']} || AUC = {maxAcc['auc']} || Precision = {maxAcc['prec']} || Recall = {maxAcc['rec']} || F1 = {maxAcc['f1']} || Accuracy = {maxAcc['acc']}\")\n",
    "\n",
    "printAccGraph(lst)\n",
    "printPrecGraph(lst)\n",
    "printRecGraph(lst)\n",
    "printF1Graph(lst)\n",
    "printAucGraph(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matriz de confusión de la cantidad de neuronas con mejores métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas predichas del modelo con la neurona con mejores metricas\n",
    "neuronas_1_predicciones = None\n",
    "for cantidad_neuronas, etiquetas_predichas in y_pred_list:\n",
    "    if cantidad_neuronas == 4:\n",
    "        neuronas_1_predicciones = etiquetas_predichas\n",
    "        break\n",
    "\n",
    "# Verificar que se encontraron las predicciones para la neurona\n",
    "if neuronas_1_predicciones is not None:\n",
    "    # Calcular la matriz de confusión\n",
    "    matriz_confusion = confusion_matrix(y_test, neuronas_1_predicciones)\n",
    "\n",
    "    # Visualizar la matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.title('Matriz de Confusión - Modelo con '+str(cantidad_neuronas)+' neuronas')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No se encontraron predicciones para la neurona en la lista.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Definir los parámetros a explorar en el GridSearch\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(i, i) for i in range(1, 100)],  # Rango de neuronas para las capas ocultas\n",
    "    'activation': ['logistic', 'relu'],  # Funciones de activación\n",
    "    'solver': ['lbfgs', 'adam'],  # Algoritmo de optimización\n",
    "    'max_iter': [20000],  # Número máximo de iteraciones\n",
    "    'random_state': [123]  # Semilla aleatoria\n",
    "}\n",
    "\n",
    "# Inicializar el clasificador MLP\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Inicializar el GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Realizar el GridSearchCV con los datos de entrenamiento\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros y el mejor score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(best_params)\n",
    "print(f\"Mejor score encontrado: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga del dataset \"The Spotify Hit Predictor Dataset (1960-2019)\" específico del 2010 al 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spotify = pd.read_csv('dataset-of-10s.csv')\n",
    "print(\"data head\")\n",
    "display(data_spotify.head())\n",
    "print(\"data info\")\n",
    "display(data_spotify.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los datos\n",
    "\n",
    "Target: Según si apareció en el TOP 100 de Billboard (al menos una vez)\n",
    "\n",
    "1 = HIT\n",
    "\n",
    "0 = FLOP\n",
    "\n",
    "Danceability: \"Vibe\" de bailabilidad, 0 a 1\n",
    "\n",
    "Energy: Intensidad de la canción, 0 a 1\n",
    "\n",
    "Key: llave/nota (-1 cuando no encuentra), positivos y 0\n",
    "\n",
    "Loudness: Qué tan ruidosa/alta es la canción, valores en dB -60 a 0\n",
    "\n",
    "Mode: Modalidad del track, escala de 1 (mayor) o 0 (menor) (discreto)\n",
    "\n",
    "Speechiness: Qué tanta lírica en el track, 0 a 1\n",
    "\n",
    "*    < 0.33: canciones melódicas, instrumentales\n",
    "*    \\> 0.33 y < 0.66: canciones normales con letra\n",
    "*    \\> 0.66: canciones de solo letra, podcasts, etc...\n",
    "\n",
    "Accousticness: Medida de confianza sobre si la canción es acústica, de 0 a 1\n",
    "\n",
    "Instrumentalness: Predice si la canción no tiene voces, de 0 a 1\n",
    "\n",
    "Liveness: Si hay audiencia en el track, 0 a 1\n",
    "\n",
    "Valence: Medida que describe el positivismo de la canción, 0 a 1\n",
    "\n",
    "Tempo: Tempo del track en bits/min, bits/min puede ir de los 10, 200...\n",
    "\n",
    "Duration in ms: Duración del track en milisegundos\n",
    "\n",
    "Time Signature: Métrica musical (no entendí 🤯😵)\n",
    "\n",
    "Chorus Hit: Mejor estimado de cuando el autor inicia el coro, es un timestamp de la canción\n",
    "\n",
    "Sections: Número de secciones que una canción tiene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos de disperción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ver mejor y label\n",
    "\n",
    "sns.scatterplot(x='danceability', y='duration_ms', hue='target', data=data_spotify, )\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Bailabilidad\")\n",
    "plt.xlabel(\"Bailabilidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='danceability', y='sections', hue='target', data=data_spotify, )\n",
    "# Mostrar el gráfico\n",
    "plt.title(\"Gráfico de Dispersión de Bailabilidad\")\n",
    "plt.xlabel(\"Bailabilidad\")\n",
    "plt.ylabel(\"Número de secciones\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='energy', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Energía\")\n",
    "plt.xlabel(\"Energía\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='key', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Llave (Nota)\")\n",
    "plt.xlabel(\"Llave (notación numérica)\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='loudness', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Ruidosidad\")\n",
    "plt.xlabel(\"Nivel de Ruido\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='mode', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Modalidad (Mayor y Menor)\")\n",
    "plt.xlabel(\"Modalidad (notación numérica)\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='speechiness', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Nivel de Lírica\")\n",
    "plt.xlabel(\"Índice de letra en la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='acousticness', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Acústica\")\n",
    "plt.xlabel(\"Índice de acusticidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='instrumentalness', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Instrumentalidad\")\n",
    "plt.xlabel(\"Índice de instrumentalidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='liveness', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Canciones en vivo\")\n",
    "plt.xlabel(\"Confiabilidad de canción en vivo\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='valence', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Valencia\")\n",
    "plt.xlabel(\"Nivel de valencia\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='tempo', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Tempo\")\n",
    "plt.xlabel(\"Tempo de la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='time_signature', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Firma de tiempo\")\n",
    "plt.xlabel(\"Firma de tiempo de la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='chorus_hit', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Entrada al coro\")\n",
    "plt.xlabel(\"Tiempo al entrar al coro\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='sections', y='duration_ms', hue='target', data=data_spotify, )\n",
    "plt.title(\"Gráfico de Dispersión de Secciones\")\n",
    "plt.xlabel(\"Cantidad de secciones\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograma de balance de las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la cantidad de datos con Outcome 1 y 0\n",
    "outcome_counts = data_spotify['target'].value_counts()\n",
    "\n",
    "# Crear el histograma\n",
    "plt.bar(outcome_counts.index, outcome_counts.values, color=['blue', 'green'])\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.title('Cantidad de datos por tipo de Canción')\n",
    "plt.xticks([0, 1], ['Flop', 'Hit'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento & Eliminación de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a intentar usar la versión de Sci-Kit\n",
    "\n",
    "data_spot_sin_strings = data_spotify.drop(['track', 'artist', 'uri'], axis=1)\n",
    "\n",
    "transformer = RobustScaler().fit(data_spot_sin_strings)\n",
    "data_spotify_sin_outliers = transformer.transform(data_spot_sin_strings)\n",
    "\n",
    "df_spotify_sin_outliers = pd.DataFrame(data_spotify_sin_outliers, columns=data_spot_sin_strings.columns)\n",
    "\n",
    "display(df_spotify_sin_outliers.head())\n",
    "display(df_spotify_sin_outliers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula el IQR\n",
    "\n",
    "data_spotify_sin_strings = data_spotify.drop(['track', 'artist', 'uri'], axis=1)\n",
    "\n",
    "Q1 = data_spotify_sin_strings.quantile(0.25)\n",
    "Q3 = data_spotify_sin_strings.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identifica los outliers\n",
    "outliers = ((data_spotify_sin_strings < (Q1 - 1.5 * IQR)) | (data_spotify_sin_strings > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Encuentra las filas que tienen al menos un outlier\n",
    "outliers_indices = outliers.any(axis=1)\n",
    "\n",
    "# Muestra las filas con outliers\n",
    "print(\"Filas con outliers:\")\n",
    "display(data_spotify_sin_strings[outliers_indices])\n",
    "\n",
    "# Elimina los outliers\n",
    "data_spotify_sin_outliers = data_spotify_sin_strings[~outliers_indices]\n",
    "\n",
    "# Muestra el conjunto de datos sin outliers\n",
    "print(\"Data sin outliers:\")\n",
    "display(data_spotify_sin_outliers.head())\n",
    "\n",
    "# Estadísticas del conjunto de datos sin outliers\n",
    "print(\"Descripción Data sin outliers\")\n",
    "display(data_spotify_sin_outliers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='danceability', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Bailabilidad\")\n",
    "plt.xlabel(\"Bailabilidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='energy', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Energía\")\n",
    "plt.xlabel(\"Energía\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='key', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Llave (Nota)\")\n",
    "plt.xlabel(\"Llave (notación numérica)\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='loudness', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Ruidosidad\")\n",
    "plt.xlabel(\"Nivel de Ruido\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='mode', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Modalidad (Mayor y Menor)\")\n",
    "plt.xlabel(\"Modalidad (notación numérica)\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='speechiness', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Nivel de Lírica\")\n",
    "plt.xlabel(\"Índice de letra en la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='acousticness', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Acústica\")\n",
    "plt.xlabel(\"Índice de acusticidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='instrumentalness', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Instrumentalidad\")\n",
    "plt.xlabel(\"Índice de instrumentalidad\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='liveness', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Canciones en vivo\")\n",
    "plt.xlabel(\"Confiabilidad de canción en vivo\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='valence', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Valencia\")\n",
    "plt.xlabel(\"Nivel de valencia\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='tempo', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Tempo\")\n",
    "plt.xlabel(\"Tempo de la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='time_signature', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Firma de tiempo\")\n",
    "plt.xlabel(\"Firma de tiempo de la canción\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='chorus_hit', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Entrada al coro\")\n",
    "plt.xlabel(\"Tiempo al entrar al coro\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='sections', y='duration_ms', hue='target', data=data_spotify_sin_outliers, )\n",
    "plt.title(\"Gráfico de Dispersión de Secciones\")\n",
    "plt.xlabel(\"Cantidad de secciones\")\n",
    "plt.ylabel(\"Duración de la canción en ms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la cantidad de datos con Outcome 1 y 0\n",
    "outcome_counts = data_spotify_sin_outliers['target'].value_counts()\n",
    "\n",
    "# Crear el histograma\n",
    "plt.bar(outcome_counts.index, outcome_counts.values, color=['blue', 'green'])\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.title('Cantidad de datos por tipo de Canción')\n",
    "plt.xticks([0, 1], ['Flop', 'Hit'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
